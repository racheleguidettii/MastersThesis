{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a792f2c-a035-42b2-9e2e-fa2f31ed5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import astropy.io.fits as fits\n",
    "\n",
    "import pysm3\n",
    "import pysm3.units as u\n",
    "\n",
    "import healpy as hp\n",
    "\n",
    "from fgbuster import (CMB, Dust, Synchrotron,xForecast,\n",
    "                      basic_comp_sep,get_sky,\n",
    "                      get_observation, get_instrument,get_noise_realization)\n",
    "from fgbuster.visualization import corner_norm\n",
    "\n",
    "from functools import partial\n",
    "from scipy.interpolate import griddata\n",
    "from skimage.restoration import inpaint\n",
    "\n",
    "import pymaster as nmt\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/rguidetti')\n",
    "from PolAngle import *\n",
    "from beam import * \n",
    "from map import *\n",
    "from convolution import * \n",
    "from beam_class import *\n",
    "from PatchFunctions import *  # nuove funzioni in quanto quelle vecchie lavoravano con mappe NxN. Queste funzionano per mappe rettangolari Nx,Ny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8423e5ca-6d27-4ec4-892e-8bee6996d403",
   "metadata": {},
   "source": [
    "# Inizializzazione mappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e571e-f9d6-42f6-b36e-7889fef9e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nside = 64\n",
    "\n",
    "sky = get_sky(nside, 'd0s0')\n",
    "instrument = get_instrument('LiteBIRD')\n",
    "freq_maps = get_observation(instrument, sky)\n",
    "\n",
    "\n",
    "pix_area_deg = hp.pixelfunc.nside2pixarea(nside, degrees=True) # degrees\n",
    "pix_size_deg = np.sqrt(pix_area_deg) # degrees\n",
    "pix_size = pix_size_deg *60 #arcmin\n",
    "pix_size\n",
    "\n",
    "print(f\"With nside = {nside}, the pixel size is {pix_size} arcmin ({pix_size_deg} degrees)\")\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "# seleziono solo alcune frequenze per comodità (8 su 15) \n",
    "\n",
    "mappa40  = freq_maps[0, :, :]\n",
    "mappa60  = freq_maps[2, :, :]\n",
    "mappa78  = freq_maps[4, :, :]\n",
    "mappa100 = freq_maps[6, :, :]\n",
    "mappa140 = freq_maps[8, :, :]\n",
    "mappa195 = freq_maps[10, :, :]\n",
    "mappa280 = freq_maps[12, :, :]\n",
    "mappa402 = freq_maps[14, :, :]\n",
    "\n",
    "\n",
    "##############################################################\n",
    "\n",
    "I40 = mappa40[0,:]\n",
    "Q40 = mappa40[1,:]\n",
    "U40 = mappa40[2,:]\n",
    "\n",
    "I60 = mappa60[0,:]\n",
    "Q60 = mappa60[1,:]\n",
    "U60 = mappa60[2,:]\n",
    "\n",
    "I78 = mappa78[0,:]\n",
    "Q78 = mappa78[1,:]\n",
    "U78 = mappa78[2,:]\n",
    "\n",
    "I100 = mappa100[0,:]\n",
    "Q100 = mappa100[1,:]\n",
    "U100 = mappa100[2,:]\n",
    "\n",
    "I140 = mappa140[0,:]\n",
    "Q140 = mappa140[1,:]\n",
    "U140 = mappa140[2,:]\n",
    "\n",
    "I195 = mappa195[0,:]\n",
    "Q195 = mappa195[1,:]\n",
    "U195 = mappa195[2,:]\n",
    "\n",
    "I280 = mappa280[0,:]\n",
    "Q280 = mappa280[1,:]\n",
    "U280 = mappa280[2,:]\n",
    "\n",
    "I402 = mappa402[0,:]\n",
    "Q402 = mappa402[1,:]\n",
    "U402 = mappa402[2,:]\n",
    "\n",
    "np.shape(I40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59b217-9764-4a36-b5de-72cf809435f9",
   "metadata": {},
   "source": [
    "# Creazione patch\n",
    "Creazione patch e conversione 1D -> 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcc9457-c5b4-485e-9cbe-a17b7702db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch\n",
    "RA1 = -60 \n",
    "RA2 = 60\n",
    "dec1 = -70\n",
    "dec2 = -40\n",
    "idxs, resol = create_rectangular_patch([RA1, RA2], [dec1, dec2], nside)\n",
    "\n",
    "\n",
    "map_array= [I40, Q40, U40, \n",
    "            I60, Q60, U60, \n",
    "            I78, Q78, U78, \n",
    "            I100, Q100, U100, \n",
    "            I140, Q140, U140, \n",
    "            I195, Q195, U195, \n",
    "            I280, Q280, U280, \n",
    "            I402, Q402, U402 ]\n",
    "\n",
    "\n",
    "for map in map_array:\n",
    "\n",
    "    visualize_patch = map.copy()\n",
    "\n",
    "    mask = np.zeros_like(visualize_patch, dtype=bool)\n",
    "    mask[idxs] = True\n",
    "\n",
    "    #visualize_patch[~mask] = hp.UNSEEN\n",
    "\n",
    "    map[~mask] = 0.0\n",
    "    \n",
    "########################################################################################\n",
    "\n",
    "maps_2d = {}\n",
    "extent=(RA1, RA2, dec1, dec2)\n",
    "\n",
    "bands = ['40', '60', '78', '100', '140', '195', '280', '402']\n",
    "types = ['I', 'Q', 'U']\n",
    "\n",
    "'''\n",
    "for band in bands:\n",
    "    for map_type in types:\n",
    "\n",
    "\n",
    "        map_name = f\"{map_type}{band}\"\n",
    "        map_data = globals()[map_name] \n",
    "        variable_prefix = f\"{map_type}{band}\"\n",
    "        \n",
    "        CMB = f\"{map_type}_map\"\n",
    "        \n",
    "        mp_hp, ra_hp, dec_hp = convert2grid(map_data, nside, resol/3, [RA1, RA2], [dec1, dec2], method='healpy', fill_nan=True)\n",
    "        \n",
    "        maps_2d[f\"mp_hp_{variable_prefix}\"] = mp_hp\n",
    "        maps_2d[f\"ra_hp_{variable_prefix}\"] = ra_hp\n",
    "        maps_2d[f\"dec_hp_{variable_prefix}\"] = dec_hp\n",
    "        #print(f\"Stored variables for: {variable_prefix}\") \n",
    "'''\n",
    "\n",
    "for band in bands:\n",
    "    for map_type in types:\n",
    "        # Construct the map names\n",
    "        map_name = f\"{map_type}{band}\"\n",
    "        #cmb_map_name = f\"{map_type}_map\"  \n",
    "        \n",
    "        \n",
    "        map_data = globals()[map_name]\n",
    "        \n",
    "     \n",
    "        #cmb_map = globals()[cmb_map_name]\n",
    "        \n",
    "        \n",
    "        \n",
    "        mp_hp, ra_hp, dec_hp = convert2grid(map_data, nside, resol/3, [RA1, RA2], [dec1, dec2], method='healpy', fill_nan=True)\n",
    "        \n",
    "        # Store the converted maps\n",
    "        variable_prefix = f\"{map_type}{band}\"\n",
    "        maps_2d[f\"mp_hp_{variable_prefix}\"] = mp_hp #+ cmb_map\n",
    "        maps_2d[f\"ra_hp_{variable_prefix}\"] = ra_hp\n",
    "        maps_2d[f\"dec_hp_{variable_prefix}\"] = dec_hp\n",
    "        \n",
    "        # Optionally, you can print a confirmation message\n",
    "\n",
    "#####################################################################################################\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "hp.mollview(I40, title='T - 1D', fig=fig.number)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.imshow(maps_2d['mp_hp_I40'],origin='lower', extent=extent)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b030a4b6-aae6-4286-9d12-00f28442269c",
   "metadata": {},
   "source": [
    "### Inizializzazione variabili mappe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c699c8-c1b9-415a-b699-263bd467b80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ny, Nx = np.shape(maps_2d['mp_hp_I40']) \n",
    "print(Nx, Ny)\n",
    "\n",
    "X_width = Nx * pix_size/60\n",
    "Y_width = Ny * pix_size/60\n",
    "\n",
    "# color map\n",
    "c_min, c_max     = -10, 10  \n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "frequencies = [40, 60, 78, 100, 140, 195, 280, 402]\n",
    "\n",
    "I_maps = {\n",
    "    40: maps_2d['mp_hp_I40'], 60: maps_2d['mp_hp_I60'], 78: maps_2d['mp_hp_I78'],\n",
    "    100: maps_2d['mp_hp_I100'], 140: maps_2d['mp_hp_I140'], 195: maps_2d['mp_hp_I195'],\n",
    "    280: maps_2d['mp_hp_I280'], 402: maps_2d['mp_hp_I402']}\n",
    "\n",
    "\n",
    "Q_maps = {\n",
    "    40: maps_2d['mp_hp_Q40'], 60: maps_2d['mp_hp_Q60'], 78: maps_2d['mp_hp_Q78'],\n",
    "    100: maps_2d['mp_hp_Q100'], 140: maps_2d['mp_hp_Q140'], 195: maps_2d['mp_hp_Q195'],\n",
    "    280: maps_2d['mp_hp_Q280'], 402: maps_2d['mp_hp_Q402']}\n",
    "\n",
    "U_maps = {\n",
    "    40: maps_2d['mp_hp_U40'], 60: maps_2d['mp_hp_U60'], 78: maps_2d['mp_hp_U78'],\n",
    "    100: maps_2d['mp_hp_U100'], 140: maps_2d['mp_hp_U140'], 195: maps_2d['mp_hp_U195'],\n",
    "    280: maps_2d['mp_hp_U280'], 402: maps_2d['mp_hp_U402']}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df4cb6-4baf-4aba-92ca-8cd2c6491261",
   "metadata": {},
   "source": [
    "# Beam - MC\n",
    "Voglio fare varie simulazioni adcon diversi valori dei picchi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564cb2a1-3ca5-42a6-b2a7-5ffc79826eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_beam = 3        \n",
    "\n",
    "\n",
    "array_dB = np.zeros((num_beam, 4))\n",
    "\n",
    "\n",
    "for i in range(num_beam):\n",
    "    start_values = [random.uniform(-50, -5) for _ in range(4)]\n",
    "    \n",
    "    peak_error = [random.uniform(-5, -0.1) for _ in range(4)]\n",
    "    sign = np.random.choice([-1, 1], size = 4)\n",
    "    \n",
    "    array_dB[i]  = (start_values + sign * peak_error)\n",
    "        \n",
    "    print(\"Peaks = \", np.sort(array_dB[i])[::-1])\n",
    "\n",
    "    array_dB[i]  = np.sort(array_dB[i]* 1.5)[::-1]\n",
    "    \n",
    "\n",
    "# VARIABLES\n",
    "FWHM_x = 1  #degrees\n",
    "FWHM_y = 1.5  #degrees\n",
    "\n",
    "    \n",
    "ellipticity = (FWHM_x - FWHM_y) / (FWHM_x + FWHM_y)\n",
    "print(\"ellipticity = \", ellipticity)\n",
    "\n",
    "\n",
    "r1 = np.array([50000, 100000, 180000, 250000]) #angular distance of the peaks\n",
    "r = 100 # width of the rings\n",
    "\n",
    "theta = 0 # angle of rotation of the beam\n",
    "a = 1 # major axis of ellipses \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edff6347-ff60-43da-a158-5258907a7d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_beam_secpeaks(pix_size, FWHMx, FWHMy, theta, array_dB, r, r1, X, Y, a, ellipticity):\n",
    "    ''' FWHMx, FWHMy = FWHM of x,y beams\n",
    "        theta = angle of rotation of the beam (degrees)\n",
    "        array_dB = array of max values of the secondary peaks\n",
    "        r = array (same dim as array_dB) of the ANGULAR distance from the center\n",
    "        r1 = width of the rings (sec peaks)\n",
    "        X, Y = coordinates\n",
    "        a = major axis for the elliptical rings\n",
    "        ellipticity\n",
    "    '''\n",
    "    # from degrees to distance in pixels\n",
    "    r1 = r1/pix_size \n",
    "\n",
    "    # if theta is not zero, we rotate the coordinates\n",
    "    X_rotated = X * np.cos(np.radians(theta)) - Y * np.sin(np.radians(theta))\n",
    "    Y_rotated = X * np.sin(np.radians(theta)) + Y * np.cos(np.radians(theta))\n",
    "\n",
    "    X = X_rotated\n",
    "    Y = Y_rotated\n",
    "\n",
    "    # MAIN BEAM ###################################################################\n",
    "    wx = FWHMx / np.sqrt(8 * np.log(2))\n",
    "    wy = FWHMy / np.sqrt(8 * np.log(2))\n",
    "\n",
    "    beam_x = np.exp(-2 * (X**2 / wx**2 + Y**2 / wy**2))\n",
    "    beam_y = np.exp(-2 * (Y**2 / wx**2 + X**2 / wy**2))\n",
    "\n",
    "\n",
    "    # SECONDARY RINGS ############################################################\n",
    "    sec_rings_x = np.zeros_like(X)\n",
    "    sec_rings_y = np.zeros_like(Y)\n",
    "\n",
    "    for i in range(len(array_dB)):\n",
    "        # Correction of ellipticity values or the secondary rings turn out flattened. If we want a circle (ell = 1) the correction is not valid\n",
    "        if (ellipticity ==1):\n",
    "            a = a\n",
    "            b = a * (ellipticity)\n",
    "        else:\n",
    "            a = a\n",
    "            b = a * (ellipticity-0.5)\n",
    "\n",
    "        distance_x = np.sqrt((X / b)**2 + (Y / a)**2)  # ellisse x\n",
    "        distance_y = np.sqrt((X / a)**2 + (Y / b)**2)  # ellisse y\n",
    "        \n",
    "        normalized_distance_x = (distance_x - r1[i]) / r\n",
    "        normalized_distance_y = (distance_y - r1[i]) / r\n",
    "        \n",
    "        max_value = 10**(array_dB[i] / 10)\n",
    "        \n",
    "        gaussian_distribution_x = np.exp(-(normalized_distance_x)**2 / 0.8)\n",
    "        gaussian_distribution_y = np.exp(-(normalized_distance_y)**2 / 0.8)\n",
    "        \n",
    "        sec_rings_x += max_value * gaussian_distribution_x\n",
    "        sec_rings_y += max_value * gaussian_distribution_y\n",
    "        \n",
    "        \n",
    "\n",
    "    # TOT BEAM ###################################################################\n",
    "    beam_x_real = beam_x + sec_rings_x\n",
    "    beam_y_real = beam_y + sec_rings_y\n",
    "   # print(sec_rings_x[49][190:220])\n",
    "    \n",
    "    \n",
    "\n",
    "    # NORMALIZATION #############################################################\n",
    "    #beam_x_real /= np.sum(beam_x_real)\n",
    "    #beam_y_real /= np.sum(beam_y_real)\n",
    "    #print(beam_x_real[49][190:220])\n",
    "    \n",
    "    return beam_x, beam_y, sec_rings_x, sec_rings_y, beam_x_real, beam_y_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0b0ecc-dd87-49b4-bc17-15e2f6d0ecba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4201edc8-c01e-407d-bd3c-d07a0f5df310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_systematics_beams_r(Nx, Ny,pix_size,beam_size_fwhp, beam,bs):\n",
    "    \n",
    "        # intitalize the beam to zero\n",
    "        B_TT=B_QQ=B_UU=B_QT=B_UT=B_QU=B_UQ=0.\n",
    "    \n",
    "        ## merge this into the beam\n",
    "        B_TT += beam\n",
    "        B_QQ += beam\n",
    "        B_UU += beam\n",
    "    \n",
    "        beam_peak = np.max(beam)\n",
    "    \n",
    "        # make the little buddies\n",
    "        Budy_TT,Budy_QT,Budy_UT = make_little_buddies_r(Nx, Ny,pix_size,beam_size_fwhp,bs,beam_peak)\n",
    "        ## merge this into the beam\n",
    "        B_TT += Budy_TT\n",
    "        B_QT += Budy_QT\n",
    "        B_UT += Budy_UT\n",
    "        \n",
    "        \n",
    "        # make the ghosting shelf\n",
    "        shelf = make_ghosting_beam_r(Nx, Ny,pix_size,beam_size_fwhp,bs,beam_peak)\n",
    "        ## merge this into the beam\n",
    "        B_TT += shelf\n",
    "        B_QQ += shelf\n",
    "        B_UU += shelf\n",
    "    \n",
    "        # make the cross talk beam\n",
    "        # make a hex grid centered on the beam\n",
    "        hex_grid = make_cross_talk_beam_grid_r(Nx,Ny,pix_size,beam_size_fwhp,bs)\n",
    "        #convolve the hex grid with the beam\n",
    "        cross_talk = convlolve(hex_grid,beam)\n",
    "        ## merge this into the beam\n",
    "        B_TT += cross_talk\n",
    "        B_QQ += cross_talk\n",
    "        B_UU += cross_talk\n",
    "        B_QT += cross_talk\n",
    "        B_UT += cross_talk\n",
    "    \n",
    "        '''\n",
    "        ## add the monopole + dipole + quadrupole T->P leakages\n",
    "        # make the beam modes\n",
    "        mono,dip_x,dip_y,quad_x,quad_45 = make_monopole_dipole_quadrupole(N,pix_size,beam_size_fwhp,bs)\n",
    "        TtoQ = bs[\"TtoQ\"][\"mono\"] * mono\n",
    "        TtoQ += bs[\"TtoQ\"][\"dip_x\"] * dip_x\n",
    "        TtoQ += bs[\"TtoQ\"][\"dip_y\"] * dip_y\n",
    "        TtoQ += bs[\"TtoQ\"][\"quad_x\"] * quad_x\n",
    "        TtoQ += bs[\"TtoQ\"][\"quad_45\"] * quad_45\n",
    "        TtoU = bs[\"TtoU\"][\"mono\"] * mono\n",
    "        TtoU += bs[\"TtoU\"][\"dip_x\"] * dip_x\n",
    "        TtoU += bs[\"TtoU\"][\"dip_y\"] * dip_y\n",
    "        TtoU += bs[\"TtoU\"][\"quad_x\"] * quad_x\n",
    "        TtoU += bs[\"TtoU\"][\"quad_45\"] * quad_45\n",
    "        ## add to the beams\n",
    "        #B_QT += TtoQ\n",
    "        #B_UT += TtoU\n",
    "        '''\n",
    "    \n",
    "        return(B_TT,B_QQ,B_UU,B_QT,B_UT,B_QU,B_UQ, Budy_TT)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e47b8-3317-42c6-ac81-6f50e3e3d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################\n",
    "\n",
    "# COORDINATES LIKE THE ONES OF THE MAPS\n",
    "\n",
    "Nx = int(Nx)\n",
    "Ny = int(Ny)\n",
    "\n",
    "onesx = np.ones(Nx)\n",
    "onesy = np.ones(Ny)\n",
    "\n",
    "indsx = (np.arange(Nx)+.5 - Nx/2.) * pix_size\n",
    "indsy = (np.arange(Ny)+.5 - Ny/2.) * pix_size\n",
    "\n",
    "X = np.outer(onesy, indsx)\n",
    "Y = np.outer(indsy, onesx)\n",
    "R = np.sqrt(X**2. + Y**2.)\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "# BEAM\n",
    "\n",
    "beam_x = {}\n",
    "\n",
    "for i in range(num_beam):\n",
    "        _, _, _,_, beam_x[i], _ = create_beam_secpeaks(pix_size, FWHM_x, FWHM_y, theta, array_dB[i], r, r1, X, Y, a, ellipticity)\n",
    "        \n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "\n",
    "### beam systematics dictionary\n",
    "bs = {\"budy\":{\"A\":3e-1,\"FWHP\":100,\"R\":1100.,\"psi\":0.3,\"polfracQ\":0.5,\"polfracU\":0.01},  \n",
    "                                                                         # little budy amplitude, \n",
    "                                                                         #FWHP, offset spacing, rotationa angle (radians)\n",
    "                                                                        # pol_fraction for Q and U\n",
    "     \"ghostshelf\": {\"A\":1e-2,\"Diam\":200.,\"roll_off\":7.},    #model of ghosting, amplitude (A), diameter (Diam) \n",
    "     \"hex_crostalk\":{\"grid_space\": 100.,\"N\":10,\"neighbor_exp_fall\":0.05}, \n",
    "                                                                               ## model of optical cross talk \n",
    "                                                                               ## to detectors on a hex grid\n",
    "                                                                               ## assumed to be exponetial\n",
    "                                                                               ## assumed to be 50% polarized\n",
    "      \"TtoQ\":{\"mono\":1e-3,\"dip_x\":1e-2,\"dip_y\":1e-2,\"quad_x\":1e-2,\"quad_45\":1e-2},\n",
    "      \"TtoU\":{\"mono\":1e-3,\"dip_x\":1e-2,\"dip_y\":1e-2,\"quad_x\":1e-2,\"quad_45\":1e-2},\n",
    "                                                                  ## multiplole expansion leakage\n",
    "      'psi':0.01*np.pi/180.\n",
    "                                  ## detetor angle rotations\n",
    "                   }\n",
    "\n",
    "beam_sys = BeamSystematics()\n",
    "\n",
    "beam_TT = {}\n",
    "beam_QQ = {}\n",
    "beam_UU = {}\n",
    "Budy_TT = {}\n",
    "for i in range(num_beam):\n",
    "    beam_TT[i], beam_QQ[i], beam_UU[i], _, _, _, _, Budy_TT[i] = make_systematics_beams_r(Nx, Ny, pix_size, FWHM_x, beam_x[i], bs)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a6a542-4322-4f70-bc34-1779f129571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.log10(beam_TT[0]))\n",
    "plt.colorbar()\n",
    "#plt.xlim(150, 250)\n",
    "#plt.ylim(20,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a35be3-cc5f-4230-8a11-c3f78d56c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution with r[eal beam\n",
    "Q_conv_real_all = {}\n",
    "U_conv_real_all = {}\n",
    "I_conv_real_all = {}\n",
    "    \n",
    "for freq in frequencies:\n",
    "    Q_map = Q_maps[freq]\n",
    "    U_map = U_maps[freq]\n",
    "    I_map = I_maps[freq]\n",
    "\n",
    "    I_conv_real  = np.zeros((num_beam, *I_map.shape))\n",
    "    Q_conv_real  = np.zeros((num_beam, *Q_map.shape))\n",
    "    U_conv_real  = np.zeros((num_beam, *U_map.shape))\n",
    "\n",
    "    \n",
    "    for i in range(num_beam):\n",
    "        I_conv_real[i, :, :] = convolve_map_with_beam(I_map, beam_TT[i])\n",
    "        Q_conv_real[i, :, :] = convolve_map_with_beam(Q_map, beam_QQ[i])\n",
    "        U_conv_real[i, :, :] = convolve_map_with_beam(U_map, beam_UU[i])\n",
    "        \n",
    "    I_conv_real_all[freq] = I_conv_real\n",
    "    Q_conv_real_all[freq] = Q_conv_real\n",
    "    U_conv_real_all[freq] = U_conv_real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e3d757-b42e-4a95-a675-ce95cb6525c5",
   "metadata": {},
   "source": [
    "# APODIZZAZIONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0519ffd-6575-4eeb-83a4-a0ebabdfba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = (cosine_window_r(Nx, Ny, x_range=[-np.pi /4, np.pi /4], y_range=[-np.pi /4, np.pi /4]))\n",
    "\n",
    "for freq in frequencies:\n",
    "    Q_conv_real_all[freq] *= window\n",
    "    U_conv_real_all[freq] *= window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd82c326-aab8-4749-ad35-b72a4da64383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot - 1 frequenza (40) per un angolo (1°) per una simulazione (1°)\n",
    "Map_to_Plot = [maps_2d['mp_hp_Q40'], Q_conv_real_all[40][0], maps_2d['mp_hp_U40'], U_conv_real_all[40][0]]\n",
    "title = [\"Q original\", \"Q conv+ap\", \"U original\", \"U conv+ap\"]\n",
    "c_factor_max = [np.max(Map_to_Plot[0]), np.max(Map_to_Plot[1]), np.max(Map_to_Plot[2]), np.max(Map_to_Plot[3])]\n",
    "c_factor_min = [np.min(Map_to_Plot[0]), np.min(Map_to_Plot[1]), np.min(Map_to_Plot[2]), np.min(Map_to_Plot[3])]\n",
    "\n",
    "tipo = 'coords'\n",
    "fig, axs = plt.subplots(2, 2, figsize=(25, 8))\n",
    "axs      = axs.flatten()\n",
    "for i, (map_data, c_factor_iter_min, c_factor_iter_max) in enumerate(zip(Map_to_Plot, c_factor_min, c_factor_max)):\n",
    "        Plot_CMB_Map_compact(axs[i], map_data, c_factor_iter_min, c_factor_iter_max, X_width, Y_width, extent, tipo)\n",
    "        axs[i].set_title(title[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.25, hspace=0.2, top=0.90)\n",
    "plt.suptitle(f'Q and U maps at {frequencies[0]} GHz, beam {1}', fontsize=20)\n",
    "#plt.savefig('/Users/guide/Documents/University/TESI/immagini/FullModelPerturbation.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3371023-4f5a-413d-92ce-b6ddff6ef863",
   "metadata": {},
   "source": [
    "# COMPONENT SEPARATION\n",
    "Trasformo in 1D, unisco le 8 frequenza per ogni simulazione, component separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d3b936-86f3-484a-9360-ef84c2811545",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = hp.ang2pix(nside, ra_hp, dec_hp, lonlat=True)\n",
    "\n",
    "# Dizionari per memorizzare le mappe piatte\n",
    "Q_flat_all = {}\n",
    "U_flat_all = {}\n",
    "I_flat_all = {}\n",
    "\n",
    "for freq in frequencies:\n",
    "    Q_map_conv = Q_conv_real_all[freq]\n",
    "    U_map_conv = U_conv_real_all[freq]\n",
    "\n",
    "    num_angles, height, width = Q_map_conv.shape #num_sim_per_angle\n",
    "\n",
    "    Q_map_flat = np.zeros((num_beam, *I40.shape)) #num_sim_per_angle\n",
    "    U_map_flat = np.zeros((num_beam, *I40.shape))\n",
    "\n",
    "    for i in range(num_beam):\n",
    "        #for j in range(num_sim_per_angle):\n",
    "        new_map_Q = np.zeros_like(I40)\n",
    "        new_map_Q[idx] = Q_map_conv[i].flatten()\n",
    "        Q_map_flat[i] = new_map_Q\n",
    "\n",
    "        new_map_U = np.zeros_like(I40)\n",
    "        new_map_U[idx] = U_map_conv[i].flatten()\n",
    "        U_map_flat[i] = new_map_U\n",
    "\n",
    "    Q_flat_all[freq] = Q_map_flat\n",
    "    U_flat_all[freq] = U_map_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58552755-2f84-40cf-aab9-0314928ea1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_array = np.zeros((len(frequencies), 2, num_angles, *I40.shape)) #num_sim_per_angle\n",
    "\n",
    "for f_idx, freq in enumerate(frequencies):\n",
    "    Q_stack = Q_flat_all[freq]\n",
    "    U_stack = U_flat_all[freq]\n",
    "\n",
    "    freq_stack = np.stack([Q_stack, U_stack], axis=0)\n",
    "\n",
    "    full_array[f_idx] = freq_stack\n",
    "    \n",
    "print(full_array.shape)  # Expected shape: (8, 2, 3, 49152)\n",
    "\n",
    "full_array = full_array.transpose(2, 0, 1, 3)\n",
    "print(full_array.shape)  # Expected shape: (3, 8, 2, 49152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c0295-0a1d-4ce8-b2df-de0795b2516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mappa Q, 40 GHz,  angolo 1\n",
    "hp.mollview(full_array[0][0][0], title=f'Q, 40 GHz, beam 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc402f16-79ba-4e21-bb0a-6d133d96284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument = get_instrument('LiteBIRD')\n",
    "\n",
    "df   = pd.DataFrame(instrument)\n",
    "instrument_freq = df.iloc[[0, 2, 4, 6, 8, 10, 12, 14], :]   #estraggo le frequenze che ho selezionato creando le patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98acac80-4f5a-4f18-ac9e-343c025f0efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [CMB(), Dust(350.), Synchrotron(20.)]\n",
    "results = []\n",
    "cmb_Q = {}\n",
    "cmb_U = {}\n",
    "cmb_I = {}\n",
    "\n",
    "for i in range(num_beam):  \n",
    "    #for j in range(num_sim_per_angle):\n",
    "\n",
    "        result = basic_comp_sep(components, instrument_freq, full_array[i]) #sim_idx\n",
    "\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "        print(f\"Simulation {i+1} - beam {i+1}\")\n",
    "        print(result.x)\n",
    "        print('\\n')\n",
    "\n",
    "    \n",
    "        if i not in cmb_Q:\n",
    "            cmb_Q[i] = {}\n",
    "            cmb_U[i] = {}\n",
    "            cmb_I[i] = {}\n",
    "\n",
    "        cmb_Q[i] = result.s[0, 0] #[j]\n",
    "        cmb_U[i] = result.s[0, 1] #[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e8205-93f8-4aa2-9b8f-206b13d10a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "hp.mollview(cmb_Q[0], title='CMB Q - angle 1', norm='hist', fig=fig.number)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "hp.mollview(cmb_U[0], title='CMB U - angle 1', norm='hist', fig=fig.number)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a283a9-d660-41b7-b96a-3e64f93dd156",
   "metadata": {},
   "source": [
    "# DECONVOLUTION\n",
    "Torno in 2D, deconvolvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5848c00-1b5a-4afb-abb6-d960c8593008",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmb_Q_2d = {}\n",
    "cmb_U_2d = {}\n",
    "cmb_I_2d = {}\n",
    "\n",
    "def safe_convert2grid(data, nside, resol, ra_range, dec_range, method, fill_nan):\n",
    "    if data.size == 0:\n",
    "        raise ValueError(\"Input data is empty.\")\n",
    "    \n",
    "    reproj_map, ra_hp, dec_hp = convert2grid(data, nside, resol, ra_range, dec_range, method, fill_nan)\n",
    "    \n",
    "    if np.isnan(reproj_map).all():\n",
    "        raise ValueError(\"Reprojected map is all NaNs.\")\n",
    "    \n",
    "    return reproj_map, ra_hp, dec_hp\n",
    "\n",
    "for i in range(num_beam):\n",
    "    #cmb_Q_2d[i] = {}\n",
    "    #cmb_U_2d[i] = {}\n",
    "    #cmb_I_2d[i] = {}\n",
    "    \n",
    "    #for j in range(num_sim_per_angle):\n",
    "    \n",
    "    cmb_Q_2d[i], ra_hp, dec_hp = safe_convert2grid(cmb_Q[i], nside, resol/3, [RA1, RA2], [dec1, dec2], method='healpy', fill_nan=True)\n",
    "    cmb_U_2d[i], ra_hp, dec_hp = safe_convert2grid(cmb_U[i], nside, resol/3, [RA1, RA2], [dec1, dec2], method='healpy', fill_nan=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb9ffbe-7560-4cd0-99fb-ab8fabd430db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perfect beam \n",
    "perf_beam = {}\n",
    "for i in range(num_beam):\n",
    "    perf_beam[i], _, _, _, _, _ = create_beam_secpeaks(pix_size, FWHM_x, FWHM_y, 0, array_dB[i], r, r1, X, Y, a, ellipticity)\n",
    "    perf_beam[i] = perf_beam[i] / np.sum(perf_beam[i])\n",
    "    \n",
    "\n",
    "Q_deconv = {}\n",
    "U_deconv = {}\n",
    "I_deconv = {}\n",
    "\n",
    "\n",
    "for i in range(num_beam):\n",
    "    #Q_deconv[i] = {}\n",
    "    #U_deconv[i] = {}\n",
    "    #I_deconv[i] = {}\n",
    "    \n",
    "    #for j in range(num_sim_per_angle):\n",
    "\n",
    "    Q_deconv[i] =  deconvolve_map(cmb_Q_2d[i], beam_x[i], perf_beam[i]) #cmb_Q_2d[i][j]\n",
    "    U_deconv[i] =  deconvolve_map(cmb_U_2d[i], beam_x[i], perf_beam[i]) #cmb_U_2d[i][j]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a93a98-71d8-4535-a9e9-97ae6e6187b6",
   "metadata": {},
   "source": [
    "# POWER SPECTRA \n",
    "Con Namaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a62004-48b9-4e64-b426-81f631e58fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pix_size_rad = np.radians(pix_size /60)\n",
    "\n",
    "X_width_rad = Nx * pix_size_rad\n",
    "Y_width_rad = Ny * pix_size_rad\n",
    "\n",
    "\n",
    "# Masks:\n",
    "mask = np.ones_like(Q_deconv[0])\n",
    "\n",
    "# bins\n",
    "\n",
    "\n",
    "l0_bins = np.arange(Nx/8) * 8 * np.pi/X_width_rad\n",
    "lf_bins = (np.arange(Nx/8)+1) * 8 * np.pi/X_width_rad\n",
    "\n",
    "\n",
    "b = nmt.NmtBinFlat(l0_bins, lf_bins)\n",
    "\n",
    "# The effective sampling rate for these bandpowers can be obtained calling:\n",
    "ells_uncoupled = b.get_effective_ells()        \n",
    "        \n",
    "BB_PS= {}\n",
    "EE_PS= {}        \n",
    "        \n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(num_beam)):\n",
    "    #TT_PS[i] = {}\n",
    "    #BB_PS[i] = {}\n",
    "    #EE_PS[i] = {}\n",
    "    #for j in range (num_sim_per_angle):\n",
    "        \n",
    "        #I = I_deconv[i][j]\n",
    "    Q = Q_deconv[i]\n",
    "    U = U_deconv[i]\n",
    "        \n",
    "    f2 = nmt.NmtFieldFlat(X_width_rad, Y_width_rad, mask, [Q, U], purify_b=True)\n",
    "        #f0 = nmt.NmtFieldFlat(X_width_rad, Y_width_rad, mask, [I])\n",
    "        \n",
    "        #w00 = nmt.NmtWorkspaceFlat()\n",
    "        #w00.compute_coupling_matrix(f0, f0, b)\n",
    "        #w02 = nmt.NmtWorkspaceFlat()\n",
    "        #w02.compute_coupling_matrix(f0, f2, b)\n",
    "    w22 = nmt.NmtWorkspaceFlat()\n",
    "    w22.compute_coupling_matrix(f2, f2, b)\n",
    "   \n",
    "        #cl22_coupled = nmt.compute_coupled_cell_flat(f2, f2, b)\n",
    "    \n",
    "        #cl00_coupled = nmt.compute_coupled_cell_flat(f0, f0, b)\n",
    "        #cl00_uncoupled = w00.decouple_cell(cl00_coupled)\n",
    "        #cl02_coupled = nmt.compute_coupled_cell_flat(f0, f2, b)\n",
    "        #cl02_uncoupled = w02.decouple_cell(cl02_coupled)\n",
    "    cl22_coupled = nmt.compute_coupled_cell_flat(f2, f2, b)\n",
    "    cl22_uncoupled = w22.decouple_cell(cl22_coupled)\n",
    "\n",
    "        #TT_PS[i][j] = cl00_uncoupled[0]\n",
    "        #EE_PS[i][j] = cl22_uncoupled[0]\n",
    "        #BB_PS[i][j] = cl22_uncoupled[3]\n",
    "    EE_PS[i] = cl22_coupled[0]\n",
    "    BB_PS[i] = cl22_coupled[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048ed3a-97af-43cf-b9d3-68f70215eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18, 5))\n",
    "factor = ells_uncoupled * (ells_uncoupled + 1) / (2 * np.pi)\n",
    "\n",
    "colors = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9']\n",
    "\n",
    "for i in range(num_beam):\n",
    "    \n",
    "    # Plot EE power spectra\n",
    "    ax[0].set_title(f'EE Power Spectra', fontsize=12)\n",
    "    \n",
    "    #for j in range(num_sim_per_angle):\n",
    "    color = colors[i % len(colors)]\n",
    "    ax[0].plot(ells_uncoupled, EE_PS[i] * factor, color=color, linewidth=1, label=f'Beam {i+1}: ({\", \".join([f\"{x:.2f}\" for x in array_dB[i]])}) deg')\n",
    "    \n",
    "    ax[0].loglog()\n",
    "    ax[0].legend(fontsize=9,loc=\"lower right\")\n",
    "    ax[0].set_xlim(1.5, 500)\n",
    "\n",
    "    # Plot BB power spectra\n",
    "    ax[1].set_title(f'BB Power Spectra', fontsize=12)\n",
    "    #for j in range(num_sim_per_angle):\n",
    "    color = colors[i % len(colors)]\n",
    "    ax[1].plot(ells_uncoupled, BB_PS[i] * factor, color=color, linewidth=1, label=f'Beam {i+1}: ({\", \".join([f\"{x:.2f}\" for x in array_dB[i]])}) deg')\n",
    "    \n",
    "    ax[1].loglog()\n",
    "    ax[1].legend(fontsize=9, loc=\"lower right\")\n",
    "    ax[1].set_xlim(1.5, 500)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a7a85d-8b4b-42fc-a8a9-7745ebfb40c4",
   "metadata": {},
   "source": [
    "# r FORECAST\n",
    "Ho cambiato la funzione in modo che non stampi niente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8eee0-e5ab-4093-b337-dd7c1a75e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosmological_likelihood(Cls_, fsky, r_input, fisher):\n",
    "    import bisect\n",
    "    from fgbuster.cosmology import _get_Cl_cmb\n",
    "    \"\"\"\n",
    "    This function returns the c.l. from\n",
    "    the likelihood on r provided Cls\n",
    "    :param Cls_: Cls^stat from comp sep\n",
    "            note: they must start from l=2\n",
    "    :param fsky: percentage of the sky to use\n",
    "    :param r_input: assumed value of r\n",
    "    :param fisher: return 68% confidence level on r or sigma(r) from fisher\n",
    "    :return sigma_r: 68% confidence level on r or sigma from fisher\n",
    "    \"\"\"\n",
    "    lmax = len(Cls_)\n",
    "    ell = np.arange(2, lmax + 2)\n",
    "\n",
    "    # values of r over which the likelihood is computed\n",
    "    if r_input >= 0.005:\n",
    "        r_max = 2*r_input\n",
    "    else:\n",
    "        r_max = 0.01\n",
    "    r_v = np.linspace(0.000, r_max, num=10 ** 6)\n",
    "    r_v = r_v.tolist()\n",
    "    # add r_input in r_v\n",
    "    if r_input not in r_v:\n",
    "        bisect.insort(r_v, r_input)\n",
    "    r_v = np.array(r_v)\n",
    "\n",
    "    ClBBr1 = _get_Cl_cmb(0.0, 1.0)[2][2:lmax + 2]\n",
    "    ClBBlens = _get_Cl_cmb(1.0, 0.0)[2][2:lmax + 2]\n",
    "    Data = ClBBlens + Cls_ + _get_Cl_cmb(0.0, r_input)[2][2:lmax+2]\n",
    "\n",
    "    if fisher:\n",
    "        # computing sigma(r=r_input) from fisher\n",
    "        F = np.sum((2 * ell + 1) * fsky / 2 * ClBBr1 ** 2 / Data ** 2)\n",
    "        sigma_r_fisher = np.sqrt(1.0 / F)\n",
    "\n",
    "        return sigma_r_fisher\n",
    "\n",
    "    else:\n",
    "        # gridding the likelihood\n",
    "        logL = np.array([np.sum((2 * ell + 1) / 2 * fsky * (np.log(ClBBr1 * r_ + ClBBlens + Cls_)\n",
    "                                                                + Data / (ClBBr1 * r_ + ClBBlens + Cls_))) for r_ in r_v])\n",
    "        L = np.exp(-(logL - np.min(logL)))\n",
    "        # computing sigma(r=r_input) as the 68% confidence level on r\n",
    "        rs_pos = r_v[r_v > r_input]\n",
    "        plike_pos = L[r_v > r_input]\n",
    "        cum = np.cumsum(plike_pos)\n",
    "        cum /= cum[-1]\n",
    "        sigma_r = rs_pos[np.argmin(np.abs(cum - 0.68))]\n",
    "\n",
    "    return sigma_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621fb752-a9fe-42a6-9cb7-46a3a31b1f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xForecast(components, instrument, d_fgs, lmin, lmax,\n",
    "              Alens=1.0, r=0.001, make_figure=False,\n",
    "              **minimize_kwargs):\n",
    "    from fgbuster import  CMB, Dust, Synchrotron, algebra, mixingmatrix, observation_helpers\n",
    "    import os.path as op\n",
    "    import numpy as np\n",
    "    import pylab as pl\n",
    "    import healpy as hp\n",
    "    import scipy as sp\n",
    "    from fgbuster.algebra import comp_sep, W_dBdB, W_dB, W, _mmm, _utmv, _mmv\n",
    "    from fgbuster.mixingmatrix import MixingMatrix\n",
    "    from fgbuster.observation_helpers import standardize_instrument\n",
    "    from fgbuster.cosmology import _get_Cl_noise,_get_Cl_cmb\n",
    "    import inspect\n",
    "    from time import time\n",
    "    import six\n",
    "    import numpy as np\n",
    "    import scipy as sp\n",
    "    import numdifftools\n",
    "    from functools import reduce\n",
    "   \n",
    "    \n",
    "    # Preliminaries\n",
    "    instrument = standardize_instrument(instrument)\n",
    "    nside = hp.npix2nside(d_fgs.shape[-1])\n",
    "    n_stokes = d_fgs.shape[1]\n",
    "    n_freqs = d_fgs.shape[0]\n",
    "    invN = np.diag(hp.nside2resol(nside, arcmin=True) / (instrument.depth_p))**2\n",
    "    mask = d_fgs[0, 0, :] != 0.\n",
    "    fsky = mask.astype(float).sum() / mask.size\n",
    "    ell = np.arange(lmin, lmax+1)\n",
    "    #print('fsky = ', fsky)\n",
    "\n",
    "    ############################################################################\n",
    "    # 1. Component separation using the noise-free foregrounds templare\n",
    "    # grab the max-L spectra parameters with the associated error bars\n",
    "    #print('======= ESTIMATION OF SPECTRAL PARAMETERS =======')\n",
    "    A = MixingMatrix(*components)\n",
    "    A_ev = A.evaluator(instrument.frequency)\n",
    "    A_dB_ev = A.diff_evaluator(instrument.frequency)\n",
    "\n",
    "    x0 = np.array([x for c in components for x in c.defaults])\n",
    "    if n_stokes == 3:  # if T and P were provided, extract P\n",
    "        d_comp_sep = d_fgs[:, 1:, :]\n",
    "    else:\n",
    "        d_comp_sep = d_fgs\n",
    "\n",
    "    res = comp_sep(A_ev, d_comp_sep.T, invN, A_dB_ev, A.comp_of_dB, x0,\n",
    "                   **minimize_kwargs)\n",
    "\n",
    "    res.params = A.params\n",
    "    res.s = res.s.T\n",
    "    A_maxL = A_ev(res.x)\n",
    "    A_dB_maxL = A_dB_ev(res.x)\n",
    "    A_dBdB_maxL = A.diff_diff_evaluator(instrument.frequency)(res.x)\n",
    "\n",
    "    #print('res.x = ', res.x)\n",
    "\n",
    "    ############################################################################\n",
    "    # 2. Estimate noise after component separation\n",
    "    ### A^T N_ell^-1 A\n",
    "    #print('======= ESTIMATION OF NOISE AFTER COMP SEP =======')\n",
    "    i_cmb = A.components.index('CMB')\n",
    "    Cl_noise = _get_Cl_noise(instrument, A_maxL, lmax)[i_cmb, i_cmb, lmin:]\n",
    "\n",
    "    ############################################################################\n",
    "    # 3. Compute spectra of the input foregrounds maps\n",
    "    ### TO DO: which size for Cl_fgs??? N_spec != 1 ? \n",
    "    #print ('======= COMPUTATION OF CL_FGS =======')\n",
    "    if n_stokes == 3:  \n",
    "        d_spectra = d_fgs\n",
    "    else:  # Only P is provided, add T for map2alm\n",
    "        d_spectra = np.zeros((n_freqs, 3, d_fgs.shape[2]), dtype=d_fgs.dtype)\n",
    "        d_spectra[:, 1:] = d_fgs\n",
    "\n",
    "    # Compute cross-spectra\n",
    "    almBs = [hp.map2alm(freq_map, lmax=lmax, iter=10)[2] for freq_map in d_spectra]\n",
    "    Cl_fgs = np.zeros((n_freqs, n_freqs, lmax+1), dtype=d_fgs.dtype)\n",
    "    for f1 in range(n_freqs):\n",
    "        for f2 in range(n_freqs):\n",
    "            if f1 > f2:\n",
    "                Cl_fgs[f1, f2] = Cl_fgs[f2, f1]\n",
    "            else:\n",
    "                Cl_fgs[f1, f2] = hp.alm2cl(almBs[f1], almBs[f2], lmax=lmax)\n",
    "\n",
    "    Cl_fgs = Cl_fgs[..., lmin:] / fsky\n",
    "\n",
    "    ############################################################################\n",
    "    # 4. Estimate the statistical and systematic foregrounds residuals\n",
    "    #print('======= ESTIMATION OF STAT AND SYS RESIDUALS =======')\n",
    "\n",
    "    W_maxL = W(A_maxL, invN=invN)[i_cmb, :]\n",
    "    W_dB_maxL = W_dB(A_maxL, A_dB_maxL, A.comp_of_dB, invN=invN)[:, i_cmb]\n",
    "    W_dBdB_maxL = W_dBdB(A_maxL, A_dB_maxL, A_dBdB_maxL,\n",
    "                         A.comp_of_dB, invN=invN)[:, :, i_cmb]\n",
    "    V_maxL = np.einsum('ij,ij...->...', res.Sigma, W_dBdB_maxL)\n",
    "\n",
    "    # Check dimentions\n",
    "    assert ((n_freqs,) == W_maxL.shape == W_dB_maxL.shape[1:]\n",
    "                       == W_dBdB_maxL.shape[2:] == V_maxL.shape)\n",
    "    assert (len(res.params) == W_dB_maxL.shape[0] \n",
    "                            == W_dBdB_maxL.shape[0] == W_dBdB_maxL.shape[1])\n",
    "\n",
    "    # elementary quantities defined in Stompor, Errard, Poletti (2016)\n",
    "    Cl_xF = {}\n",
    "    Cl_xF['yy'] = _utmv(W_maxL, Cl_fgs.T, W_maxL)  # (ell,)\n",
    "    Cl_xF['YY'] = _mmm(W_dB_maxL, Cl_fgs.T, W_dB_maxL.T)  # (ell, param, param)\n",
    "    Cl_xF['yz'] = _utmv(W_maxL, Cl_fgs.T, V_maxL )  # (ell,)\n",
    "    Cl_xF['Yy'] = _mmv(W_dB_maxL, Cl_fgs.T, W_maxL)  # (ell, param)\n",
    "    Cl_xF['Yz'] = _mmv(W_dB_maxL, Cl_fgs.T, V_maxL)  # (ell, param)\n",
    "\n",
    "    # bias and statistical foregrounds residuals\n",
    "    res.noise = Cl_noise\n",
    "    res.bias = Cl_xF['yy'] + 2 * Cl_xF['yz']  # S16, Eq 23\n",
    "    res.stat = np.einsum('ij, lij -> l', res.Sigma, Cl_xF['YY'])  # E11, Eq. 12\n",
    "    res.var = res.stat**2 + 2 * np.einsum('li, ij, lj -> l', # S16, Eq. 28\n",
    "                                          Cl_xF['Yy'], res.Sigma, Cl_xF['Yy'])\n",
    "\n",
    "    ###############################################################################\n",
    "    # 5. Plug into the cosmological likelihood\n",
    "    #print ('======= OPTIMIZATION OF COSMO LIKELIHOOD =======')\n",
    "    Cl_fid = {}\n",
    "    Cl_fid['BB'] = _get_Cl_cmb(Alens=Alens, r=r)[2][lmin:lmax+1]\n",
    "    Cl_fid['BuBu'] = _get_Cl_cmb(Alens=0.0, r=1.0)[2][lmin:lmax+1]\n",
    "    Cl_fid['BlBl'] = _get_Cl_cmb(Alens=1.0, r=0.0)[2][lmin:lmax+1]\n",
    "\n",
    "    res.BB = Cl_fid['BB']*1.0\n",
    "    res.BuBu = Cl_fid['BuBu']*1.0\n",
    "    res.BlBl = Cl_fid['BlBl']*1.0\n",
    "    res.ell = ell\n",
    "    if make_figure:\n",
    "        fig = pl.figure( figsize=(14,12), facecolor='w', edgecolor='k' )\n",
    "        ax = pl.gca()\n",
    "        left, bottom, width, height = [0.2, 0.2, 0.15, 0.2]\n",
    "        ax0 = fig.add_axes([left, bottom, width, height])\n",
    "        ax0.set_title(r'$\\ell_{\\min}=$'+str(lmin)+\\\n",
    "            r'$ \\rightarrow \\ell_{\\max}=$'+str(lmax), fontsize=16)\n",
    "\n",
    "        ax.loglog(ell, Cl_fid['BB'], color='DarkGray', linestyle='-', label='BB tot', linewidth=2.0)\n",
    "        ax.loglog(ell, Cl_fid['BuBu']*r , color='DarkGray', linestyle='--', label='primordial BB for r='+str(r), linewidth=2.0)\n",
    "        ax.loglog(ell, res.stat, 'DarkOrange', label='statistical residuals', linewidth=2.0)\n",
    "        ax.loglog(ell, res.bias, 'DarkOrange', linestyle='--', label='systematic residuals', linewidth=2.0)\n",
    "        ax.loglog(ell, res.noise, 'DarkBlue', linestyle='--', label='noise after component separation', linewidth=2.0)\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('$\\ell$', fontsize=20)\n",
    "        ax.set_ylabel('$C_\\ell$ [$\\mu$K-arcmin]', fontsize=20)\n",
    "        ax.set_xlim(lmin,lmax)\n",
    "\n",
    "    ## 5.1. data \n",
    "    Cl_obs = Cl_fid['BB'] + Cl_noise\n",
    "    dof = (2 * ell + 1) * fsky\n",
    "    YY = Cl_xF['YY']\n",
    "    tr_SigmaYY = np.einsum('ij, lji -> l', res.Sigma, YY)\n",
    "\n",
    "    ## 5.2. modeling\n",
    "    def cosmo_likelihood(r_):\n",
    "        # S16, Appendix C\n",
    "        Cl_model = Cl_fid['BlBl'] * Alens + Cl_fid['BuBu'] * r_ + Cl_noise\n",
    "        dof_over_Cl = dof / Cl_model\n",
    "        ## Eq. C3\n",
    "        U = np.linalg.inv(res.Sigma_inv + np.dot(YY.T, dof_over_Cl))\n",
    "        \n",
    "        ## Eq. C9\n",
    "        first_row = np.sum(dof_over_Cl * (\n",
    "            Cl_obs * (1 - np.einsum('ij, lji -> l', U, YY) / Cl_model) \n",
    "            + tr_SigmaYY))\n",
    "        second_row = - np.einsum(\n",
    "            'l, m, ij, mjk, kf, lfi',\n",
    "            dof_over_Cl, dof_over_Cl, U, YY, res.Sigma, YY)\n",
    "        trCinvC = first_row + second_row\n",
    "       \n",
    "        ## Eq. C10\n",
    "        first_row = np.sum(dof_over_Cl * (Cl_xF['yy'] + 2 * Cl_xF['yz']))\n",
    "        ### Cyclicity + traspose of scalar + grouping terms -> trace becomes\n",
    "        ### Yy_ell^T U (Yy + 2 Yz)_ell'\n",
    "        trace = np.einsum('li, ij, mj -> lm',\n",
    "                          Cl_xF['Yy'], U, Cl_xF['Yy'] + 2 * Cl_xF['Yz'])\n",
    "        second_row = - _utmv(dof_over_Cl, trace, dof_over_Cl)\n",
    "        trECinvC = first_row + second_row\n",
    "\n",
    "        ## Eq. C12\n",
    "        logdetC = np.sum(dof * np.log(Cl_model)) - np.log(np.linalg.det(U))\n",
    "\n",
    "        return trCinvC + trECinvC + logdetC\n",
    "\n",
    "\n",
    "    # Likelihood maximization\n",
    "    r_grid = np.logspace(-5,0,num=500)\n",
    "    logL = np.array([cosmo_likelihood(r_loc) for r_loc in r_grid])\n",
    "    ind_r_min = np.argmin(logL)\n",
    "    r0 = r_grid[ind_r_min]\n",
    "    \n",
    "    if ind_r_min == 0:\n",
    "        ind_r_min = np.argmax(logL)\n",
    "        bound_0 = r_grid[ind_r_min-1]\n",
    "        bound_1 = r_grid[ind_r_min+1]\n",
    "\n",
    "    elif ind_r_min == len(r_grid)-1:\n",
    "        bound_0 = r_grid[-2]\n",
    "        bound_1 = 1.0\n",
    "\n",
    "    else:\n",
    "        bound_0 = r_grid[ind_r_min-1]\n",
    "        bound_1 = r_grid[ind_r_min+1]\n",
    "\n",
    "    res_Lr = sp.optimize.minimize(cosmo_likelihood, [r0], bounds=[(bound_0,bound_1)], **minimize_kwargs)\n",
    "\n",
    "    def sigma_r_computation_from_logL(r_loc):\n",
    "        THRESHOLD = 1.00\n",
    "        # THRESHOLD = 2.30 when two fitted parameters\n",
    "        delta = np.abs( cosmo_likelihood(r_loc) - res_Lr['fun'] - THRESHOLD )\n",
    "        return delta\n",
    "\n",
    "    if res_Lr['x'] != 0.0:\n",
    "        sr_grid = np.logspace(np.log10(res_Lr['x']), 0, num=25)\n",
    "    else:\n",
    "        sr_grid = np.logspace(-5,0,num=25)\n",
    "\n",
    "    slogL = np.array([sigma_r_computation_from_logL(sr_loc) for sr_loc in sr_grid ])\n",
    "    ind_sr_min = np.argmin(slogL)\n",
    "    sr0 = sr_grid[ind_sr_min]\n",
    "\n",
    "     \n",
    "\n",
    "    \n",
    "    if ind_sr_min == 0:\n",
    "        ind_r_min = np.argmax(logL)\n",
    "        bound_0 = sr_grid[ind_sr_min+1]\n",
    "        bound_1 = sr_grid[ind_sr_min-1]\n",
    "    elif ind_sr_min == len(sr_grid)-1:\n",
    "\n",
    "        bound_0 = sr_grid[-2]\n",
    "        bound_1 = 1.0\n",
    "    else:\n",
    "\n",
    "        bound_0 = sr_grid[ind_sr_min-1]\n",
    "        bound_1 = sr_grid[ind_sr_min+1]\n",
    "\n",
    "    #res_sr = sp.optimize.minimize(sigma_r_computation_from_logL, sr0,\n",
    "    #        bounds=[(bound_0.item(),bound_1.item())],\n",
    "    #        **minimize_kwargs)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    sigma_r = cosmological_likelihood(Cl_obs, fsky, r0, True) #<----------------------------------------------------------------------------------- NEW SIGMA\n",
    "    sigma_r = np.array([sigma_r])\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    res.cosmo_params = {}\n",
    "    res.cosmo_params['r'] = (res_Lr['x'], sigma_r) #res_sr['x']- res_Lr['x']\n",
    "\n",
    "    r_grid = np.logspace(-4,-1,num=500)\n",
    "    logL = np.array([ cosmo_likelihood(r_loc) for r_loc in r_grid ])\n",
    "    chi2 = logL - np.min(logL)\n",
    "    ###############################################################################\n",
    "    # 6. Produce figures\n",
    "    if make_figure:\n",
    "        #print ('======= GRIDDING COSMO LIKELIHOOD =======')\n",
    "        r_grid = np.logspace(-4,-1,num=500)\n",
    "        logL = np.array([ cosmo_likelihood(r_loc) for r_loc in r_grid ])\n",
    "        chi2 = logL - np.min(logL)\n",
    "        ax0.semilogx( r_grid,  np.exp(-chi2), color='DarkOrange', linestyle='-', linewidth=2.0, alpha=0.8 )\n",
    "        ax0.axvline(x=r, color='k', linestyle='--')\n",
    "        ax0.set_ylabel(r'$\\mathcal{L}(r)$', fontsize=20)\n",
    "        ax0.set_xlabel(r'tensor-to-scalar ratio $r$', fontsize=20)\n",
    "        pl.show()\n",
    "\n",
    "    return res, np.exp(-chi2), r_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527d81e3-0849-4dd4-a66d-70ab346df1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_array[0][0][0][45000:45030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e173d3-e71f-4fd0-97f6-e564802b6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_array[1][0][0][45000:45030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c78176-45b4-4b0c-84dd-060d1db7bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_array[2][0][0][45000:45030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e049adf6-067c-46cc-8657-19decdfcdfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "logL = {}\n",
    "r_grid ={}\n",
    "r = np.zeros((num_angles))\n",
    "sigma_r = np.zeros((num_angles))\n",
    "\n",
    "\n",
    "\n",
    "for i in range (num_beam): \n",
    "    res[i], logL[i], r_grid[i] = xForecast(components, instrument_freq, full_array[i], 2, 2*nside-1, Alens=0.1, r=0.001, make_figure=False) \n",
    "    res[i].cosmo_params['r'] = np.array(res[i].cosmo_params['r'])\n",
    "    r[i] = (res[i].cosmo_params['r'][0][0])\n",
    "    sigma_r[i] = (res[i].cosmo_params['r'][1][0])\n",
    "    print(f'Simulation {i+1} --> r = {r[i]:.8}, sigma_r = {sigma_r[i]:.11}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b9056-43ad-4265-b2be-9f39ed230fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"red\",\"blue\",\"green\"]\n",
    "\n",
    "for i in range (num_beam): \n",
    "    plt.semilogx( r_grid[i], logL[i], color=colors[i], linestyle='-', linewidth = 1, alpha=0.8, label=f'Beam {i+1}' )\n",
    "    \n",
    "plt.axvline(x=0.001, color='k', linestyle='--')\n",
    "    \n",
    "plt.ylabel(r'$\\mathcal{L}(r)$', fontsize=15)\n",
    "plt.xlabel(r'tensor-to-scalar ratio $r$', fontsize=15)\n",
    "plt.xlim(3e-4, 4e-3)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21548b74-90d1-4348-87b7-bc318011875c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmb",
   "language": "python",
   "name": "cmb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
